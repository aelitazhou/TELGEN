{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82db4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from ml_collections import ConfigDict\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from data.data_preprocess import HeteroAddLaplacianEigenvectorPE, SubSample\n",
    "from data.dataset import LPDataset\n",
    "from data.utils import args_set_bool, collate_fn_ip\n",
    "from models.hetero_gnn import TripartiteHeteroGNN_\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe7ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# of outer loop: ipm_steps\n",
    "# of inner loop: num_conv_layers\n",
    "\n",
    "var_dict = {\n",
    "            \"weight_decay\": 0,\n",
    "            \"micro_batch\": 4,         \n",
    "            \"batchsize\": 128,         \n",
    "            \"hidden\": 180, \n",
    "            \"num_conv_layers\": 2,     \n",
    "            \"num_pred_layers\": 4, \n",
    "            \"num_mlp_layers\": 4, \n",
    "            \"share_lin_weight\": True, \n",
    "            \"conv_sequence\": 'cov', \n",
    "            \"loss_weight_x\": 1.0, \n",
    "            \"loss_weight_obj\": 3.43, \n",
    "            \"loss_weight_cons\": 5.8,    \n",
    "            \"losstype\": 'l2',\n",
    "            \"runs\": 3,\n",
    "            \"lappe\": 0, \n",
    "            \"conv\": 'gcnconv', \n",
    "            \"epoch\": 1,  \n",
    "            \"ipm_alpha\": 0.7,\n",
    "            \"ipm_steps\": 16,   \n",
    "            \"dropout\": 0,\n",
    "            \"share_conv_weight\": True,        \n",
    "            \"use_norm\": True,\n",
    "            \"use_res\": True,  \n",
    "            \"lr\": 1.e-5,  \n",
    "            \"weight_decay\": 0\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6f689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project='xxx',\n",
    "           config=var_dict,\n",
    "           entity=\"xxx\")\n",
    "# use you own name for project='xxx' and entity=\"xxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "780ea6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if wandb.run is not None:\n",
    "    print('wandb running')\n",
    "else:\n",
    "    print('wandb not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfe6e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data for training or testing using the raw data instances in ./raw/raw folder\n",
    "# use this 'train_ins', 'test_ins' to differentiate \n",
    "\n",
    "train_ins = 'train_b4' \n",
    "test_ins = 'test_b4'\n",
    "\n",
    "ipm = 16\n",
    "train_dataset = LPDataset('raw',\n",
    "                    extra_path=f'{1}restarts_'\n",
    "                                     f'{0}lap_'\n",
    "                                     f'{ipm}steps'\n",
    "                                     f'{\"_upper_\" + str(train_ins)}',\n",
    "                    upper_bound=1,\n",
    "                    rand_starts=1,\n",
    "                    pre_transform=Compose([HeteroAddLaplacianEigenvectorPE(k=0),\n",
    "                                                 SubSample(ipm)]))\n",
    "\n",
    "test_dataset = LPDataset('raw',\n",
    "                    extra_path=f'{1}restarts_'\n",
    "                                     f'{0}lap_'\n",
    "                                     f'{ipm}steps'\n",
    "                                     f'{\"_upper_\" + str(test_ins)}',\n",
    "                    upper_bound=1,\n",
    "                    rand_starts=1,\n",
    "                    pre_transform=Compose([HeteroAddLaplacianEigenvectorPE(k=0),\n",
    "                                                 SubSample(ipm)]))\n",
    "\n",
    "# train and test on different dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=var_dict['batchsize'],\n",
    "                          shuffle=True,\n",
    "                          num_workers=1,\n",
    "                          collate_fn=collate_fn_ip)\n",
    "val_loader = DataLoader(train_dataset[int(len(train_dataset) * 0.9):],\n",
    "                        batch_size=var_dict['batchsize'],\n",
    "                        shuffle=True,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn_ip)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=var_dict['batchsize'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "961d9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb33da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2026     # 2026, 2027, 2028\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "best_val_objgap_mean = []\n",
    "best_val_consgap_mean = []\n",
    "test_objgap_mean = []\n",
    "test_consgap_mean = []\n",
    "test_objgap_nocon_mean = []\n",
    "\n",
    "for run in range(1):\n",
    "    \n",
    "    if not os.path.isdir('logs'):\n",
    "        os.mkdir('logs')\n",
    "    exist_runs = [d for d in os.listdir('logs') if d.startswith('exp')]\n",
    "    log_folder_name = f'logs/exp{len(exist_runs)}'\n",
    "    os.mkdir(log_folder_name)\n",
    "    with open(os.path.join(log_folder_name, 'config.yaml'), 'w') as outfile:\n",
    "        yaml.dump(var_dict, outfile, default_flow_style=False)\n",
    "            \n",
    "    os.mkdir(os.path.join(log_folder_name, f'run{run}'))\n",
    "\n",
    "    model = TripartiteHeteroGNN_(ipm_steps=var_dict['ipm_steps'],\n",
    "                               conv=var_dict['conv'],\n",
    "                               in_shape=2,\n",
    "                               pe_dim=var_dict['lappe'],\n",
    "                               hid_dim=var_dict['hidden'],\n",
    "                               num_conv_layers=var_dict['num_conv_layers'],\n",
    "                               num_pred_layers=var_dict['num_pred_layers'],\n",
    "                               num_mlp_layers=var_dict['num_mlp_layers'],\n",
    "                               dropout=var_dict['dropout'],\n",
    "                               share_conv_weight=var_dict['share_conv_weight'],\n",
    "                               share_lin_weight=var_dict['share_lin_weight'],\n",
    "                               use_norm=var_dict['use_norm'],\n",
    "                               use_res=var_dict['use_res'],\n",
    "                               conv_sequence=var_dict['conv_sequence']).to(device)\n",
    "    \n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=var_dict['lr'], weight_decay=var_dict['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1.e-6)\n",
    "\n",
    "    trainer = Trainer(device,\n",
    "                      'primal+objgap+constraint',\n",
    "                      var_dict['losstype'],\n",
    "                      var_dict['micro_batch'],\n",
    "                      var_dict['ipm_steps'],\n",
    "                      var_dict['ipm_alpha'],\n",
    "                      loss_weight={'primal': var_dict['loss_weight_x'],\n",
    "                                   'objgap': var_dict['loss_weight_obj'],\n",
    "                                   'constraint': var_dict['loss_weight_cons']})\n",
    "\n",
    "    pbar = tqdm(range(var_dict['epoch']))\n",
    "    curr = time.time()\n",
    "    for epoch in pbar:\n",
    "        train_loss, primal_loss, obj_loss, cons_loss = trainer.train_(train_loader, model, optimizer)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            val_gaps, val_constraint_gap, val_gaps_nocon = trainer.eval_metrics_(val_loader, model)\n",
    "\n",
    "            # metric to cache the best model\n",
    "            cur_mean_gap = val_gaps[:, -1].mean().item()\n",
    "            cur_cons_gap_mean = val_constraint_gap[:, -1].mean().item()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(cur_mean_gap)\n",
    "                \n",
    "            torch.save(model.state_dict(), os.path.join(log_folder_name, f'run{run}', str(epoch)+'_model.pt'))\n",
    "            \n",
    "            if trainer.best_val_objgap > cur_mean_gap:\n",
    "                trainer.patience = 0\n",
    "                trainer.best_val_objgap = cur_mean_gap\n",
    "                trainer.best_val_consgap = cur_cons_gap_mean\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "  \n",
    "                torch.save(model.state_dict(), os.path.join(log_folder_name, f'run{run}', str(epoch)+'_best_model.pt'))\n",
    "\n",
    "\n",
    "        pbar.set_postfix({'train_loss': train_loss,\n",
    "                          'primal_loss': primal_loss,\n",
    "                          'obj_loss': obj_loss,\n",
    "                          'cons_loss': cons_loss,\n",
    "                          'val_obj': cur_mean_gap,\n",
    "                          'val_cons': cur_cons_gap_mean,\n",
    "                          'lr': scheduler.optimizer.param_groups[0][\"lr\"]})\n",
    "        log_dict = {'train_loss': train_loss,\n",
    "                    'primal_loss': primal_loss,\n",
    "                    'obj_loss': obj_loss,\n",
    "                    'cons_loss': cons_loss,\n",
    "                    'val_obj_gap_last_mean': cur_mean_gap,\n",
    "                    'val_cons_gap_last_mean': cur_cons_gap_mean,\n",
    "                   'lr': scheduler.optimizer.param_groups[0][\"lr\"]}\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "    print('time:', time.time()-curr)\n",
    "\n",
    "    best_val_objgap_mean.append(trainer.best_val_objgap)\n",
    "    best_val_consgap_mean.append(trainer.best_val_consgap)\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_gaps, test_cons_gap, test_gaps_nocon = trainer.eval_metrics_(test_loader, model)\n",
    "\n",
    "    \n",
    "    test_objgap_mean.append(test_gaps[:, -1].mean().item())\n",
    "    test_consgap_mean.append(test_cons_gap[:, -1].mean().item())\n",
    "    test_objgap_nocon_mean.append(test_gaps_nocon[:, -1].mean().item())\n",
    "\n",
    "    wandb.log({'test_objgap': test_objgap_mean[-1]})\n",
    "    wandb.log({'test_consgap': test_consgap_mean[-1]})\n",
    "    wandb.log({'test_objgap_nocon': test_objgap_nocon_mean[-1]})\n",
    "\n",
    "\n",
    "wandb.log({\n",
    "    'best_val_objgap': np.mean(best_val_objgap_mean),\n",
    "    'test_objgap_mean': np.mean(test_objgap_mean),\n",
    "    'test_objgap_std': np.std(test_objgap_mean),\n",
    "    'test_consgap_mean': np.mean(test_consgap_mean),\n",
    "    'test_consgap_std': np.std(test_consgap_mean),\n",
    "    'test_hybrid_gap': np.mean(test_objgap_mean) + np.mean(test_consgap_mean),  # for the sweep\n",
    "})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254062c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a6cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a368400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipmgnn",
   "language": "python",
   "name": "ipmgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
