{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82db4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from ml_collections import ConfigDict\n",
    "import yaml\n",
    "import time\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from data.data_preprocess import HeteroAddLaplacianEigenvectorPE_harp, SubSample\n",
    "from data.dataset import LPDataset_harp, LPDataset\n",
    "from data.utils import args_set_bool, collate_fn_ip_harp, collate_fn_ip\n",
    "from models.hetero_gnn import QuadpartiteHeteroGNN\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fecdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb not running\n"
     ]
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    print('wandb running')\n",
    "else:\n",
    "    print('wandb not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe7ab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2*ipm_steps/num_conv_layers = outer loop\n",
    "# num_conv_layers = inner loop\n",
    "var_dict = {\n",
    "            \"weight_decay\": 0,\n",
    "            \"micro_batch\": 16,         # oringal: 4, \n",
    "            \"batchsize\": 32,          # oringal: 128,\n",
    "            \"hidden\": 180, \n",
    "            \"num_conv_layers\": 2,     # oringinal: 8,  \n",
    "            \"num_pred_layers\": 4, \n",
    "            \"num_mlp_layers\": 4, \n",
    "#             \"share_lin_weight\": 'false', \n",
    "            \"share_lin_weight\": True, \n",
    "            \"conv_sequence\": 'ceov', \n",
    "            \"loss_weight_x\": 1.0, \n",
    "            \"loss_weight_obj\": 3.43, \n",
    "            \"loss_weight_cons\": 1.8,     # oringinal: 5.8\n",
    "            \"losstype\": 'l2',\n",
    "            \"runs\": 3,\n",
    "            \"lappe\": 0, \n",
    "            \"conv\": 'gcnconv', \n",
    "            \"epoch\": 50,    # oringal: 500\n",
    "#             \"ipm_alpha\": 0.35,\n",
    "            \"ipm_alpha\": 0.7,\n",
    "    \n",
    "            \"ipm_steps\": 16,       # 8 for supervising every mlp layer, 1 for supervising last layer\n",
    "    \n",
    "            \"dropout\": 0,\n",
    "#             \"share_conv_weight\": 'false',        # default: false   \n",
    "            \"share_conv_weight\": True,        # default: false  \n",
    "#             \"use_norm\": 'true',\n",
    "#             \"use_res\": 'false',  \n",
    "            \"use_norm\": True,\n",
    "            \"use_res\": True,  \n",
    "            \"lr\": 2.e-4,     # original: 1.e-3\n",
    "            \"weight_decay\": 0\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6f689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfzhou\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/data/fzhou/TELGEN_/wandb/run-20250209_230909-tfs8r8z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fzhou/reallocation_harp/runs/tfs8r8z3' target=\"_blank\">resilient-elevator-21</a></strong> to <a href='https://wandb.ai/fzhou/reallocation_harp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fzhou/reallocation_harp' target=\"_blank\">https://wandb.ai/fzhou/reallocation_harp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fzhou/reallocation_harp/runs/tfs8r8z3' target=\"_blank\">https://wandb.ai/fzhou/reallocation_harp/runs/tfs8r8z3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fzhou/reallocation_harp/runs/tfs8r8z3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2cfa7b9de0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project='reallocation_harp',\n",
    "           config=var_dict,\n",
    "           entity=\"fzhou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780ea6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb running\n"
     ]
    }
   ],
   "source": [
    "if wandb.run is not None:\n",
    "    print('wandb running')\n",
    "else:\n",
    "    print('wandb not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a085491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ax-b=1\n",
    "# Use QaudpartitionGNN\n",
    "\n",
    "train_ins = 'train_abilene' \n",
    "valid_ins = 'valid_abilene'\n",
    "test_ins = 'test_abilene'\n",
    "ipm = 16\n",
    "\n",
    "# train_toy_ins = 'train_toy_abilene' \n",
    "# train_toy_dataset = LPDataset_harp('raw',\n",
    "#                     extra_path=f'{1}restarts_'\n",
    "#                                      f'{0}lap_'\n",
    "#                                      f'{ipm}steps'\n",
    "#                                      f'{\"_upper_\" + str(train_toy_ins)}',\n",
    "#                     upper_bound=1,\n",
    "#                     rand_starts=1,\n",
    "#                     pre_transform=Compose([HeteroAddLaplacianEigenvectorPE_harp(k=0),\n",
    "#                                                  SubSample(ipm)]))\n",
    "# train_toy_loader = DataLoader(train_toy_dataset,\n",
    "#                           batch_size=var_dict['batchsize'],\n",
    "#                           shuffle=True,\n",
    "#                           num_workers=1,\n",
    "#                           collate_fn=collate_fn_ip_harp)\n",
    "\n",
    "train_dataset = LPDataset_harp('raw',\n",
    "                    extra_path=f'{1}restarts_'\n",
    "                                     f'{0}lap_'\n",
    "                                     f'{ipm}steps'\n",
    "                                     f'{\"_upper_\" + str(train_ins)}',\n",
    "                    upper_bound=1,\n",
    "                    rand_starts=1,\n",
    "                    pre_transform=Compose([HeteroAddLaplacianEigenvectorPE_harp(k=0),\n",
    "                                                 SubSample(ipm)]))\n",
    "valid_dataset = LPDataset_harp('raw',\n",
    "                    extra_path=f'{1}restarts_'\n",
    "                                     f'{0}lap_'\n",
    "                                     f'{ipm}steps'\n",
    "                                     f'{\"_upper_\" + str(valid_ins)}',\n",
    "                    upper_bound=1,\n",
    "                    rand_starts=1,\n",
    "                    pre_transform=Compose([HeteroAddLaplacianEigenvectorPE_harp(k=0),\n",
    "                                                 SubSample(ipm)]))\n",
    "test_dataset = LPDataset_harp('raw',\n",
    "                    extra_path=f'{1}restarts_'\n",
    "                                     f'{0}lap_'\n",
    "                                     f'{ipm}steps'\n",
    "                                     f'{\"_upper_\" + str(test_ins)}',\n",
    "                    upper_bound=1,\n",
    "                    rand_starts=1,\n",
    "                    pre_transform=Compose([HeteroAddLaplacianEigenvectorPE_harp(k=0),\n",
    "                                                 SubSample(ipm)]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=var_dict['batchsize'],\n",
    "                          shuffle=True,\n",
    "                          num_workers=1,\n",
    "                          collate_fn=collate_fn_ip_harp)\n",
    "val_loader = DataLoader(valid_dataset,\n",
    "                        batch_size=var_dict['batchsize'],\n",
    "                        shuffle=True,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn_ip_harp)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=var_dict['batchsize'],\n",
    "                        shuffle=False,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn_ip_harp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43c55318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LPDataset_harp(12095), LPDataset_harp(2016), LPDataset_harp(2016))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf54bcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377.96875, 31)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)/var_dict[\"batchsize\"], len(train_dataset)%var_dict[\"batchsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e810f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)/var_dict[\"batchsize\"], len(valid_dataset)%var_dict[\"batchsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae596224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63.0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)/var_dict[\"batchsize\"], len(test_dataset)%var_dict[\"batchsize\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394b1644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 63, 63)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698b7953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad12657",
   "metadata": {},
   "source": [
    "# Quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143aa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# TripartiteHeteroGNN_, train_newnew_con: return primal, obj, con loss \n",
    "\n",
    "seed = 2028     # 2025, 2026, 2027, 2028\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "best_val_objgap_mean = []\n",
    "best_val_consgap_mean = []\n",
    "best_val_econsgap_mean = []\n",
    "\n",
    "test_objgap_mean = []\n",
    "test_consgap_mean = []\n",
    "test_objgap_nocon_mean = []\n",
    "\n",
    "for run in range(1):\n",
    "    \n",
    "    if not os.path.isdir('logs'):\n",
    "        os.mkdir('logs')\n",
    "    exist_runs = [d for d in os.listdir('logs') if d.startswith('exp')]\n",
    "    log_folder_name = f'logs/exp{len(exist_runs)}'\n",
    "    os.mkdir(log_folder_name)\n",
    "    with open(os.path.join(log_folder_name, 'config.yaml'), 'w') as outfile:\n",
    "        yaml.dump(var_dict, outfile, default_flow_style=False)\n",
    "            \n",
    "    os.mkdir(os.path.join(log_folder_name, f'run{run}'))\n",
    "\n",
    "    model = QuadpartiteHeteroGNN(ipm_steps=var_dict['ipm_steps'],\n",
    "                                 conv=var_dict['conv'],\n",
    "                                 in_shape=2,\n",
    "                                 pe_dim=var_dict['lappe'],\n",
    "                                 hid_dim=var_dict['hidden'],\n",
    "                                 num_conv_layers=var_dict['num_conv_layers'],\n",
    "                                 num_pred_layers=var_dict['num_pred_layers'],\n",
    "                                 num_mlp_layers=var_dict['num_mlp_layers'],\n",
    "                                 dropout=var_dict['dropout'],\n",
    "                                 share_conv_weight=var_dict['share_conv_weight'],\n",
    "                                 share_lin_weight=var_dict['share_lin_weight'],\n",
    "                                 use_norm=var_dict['use_norm'],\n",
    "                                 use_res=var_dict['use_res'],\n",
    "                                 conv_sequence=var_dict['conv_sequence']).to(device)\n",
    "    \n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=var_dict['lr'], weight_decay=var_dict['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1.e-6)\n",
    "\n",
    "    trainer = Trainer(device,\n",
    "                      'primal+objgap+constraint',\n",
    "                      var_dict['losstype'],\n",
    "                      var_dict['micro_batch'],\n",
    "                      var_dict['ipm_steps'],\n",
    "                      var_dict['ipm_alpha'],\n",
    "                      loss_weight={'primal': var_dict['loss_weight_x'],\n",
    "                                   'objgap': var_dict['loss_weight_obj'],\n",
    "                                   'constraint': var_dict['loss_weight_cons']})\n",
    "\n",
    "    pbar = tqdm(range(var_dict['epoch']))\n",
    "    curr = time.time()\n",
    "    for epoch in pbar:\n",
    "        train_loss, primal_loss, obj_loss, cons_loss, econs_loss = trainer.train_harp(train_loader, model, optimizer)\n",
    "        with torch.no_grad():\n",
    "            val_gaps, val_constraint_gap, val_econstraint_gap = trainer.eval_metrics_harp(val_loader, model)\n",
    "\n",
    "            # metric to cache the best model\n",
    "            cur_mean_gap = val_gaps[:, -1].mean().item()\n",
    "            cur_cons_gap_mean = val_constraint_gap[:, -1].mean().item()\n",
    "            cur_econs_gap_mean = val_econstraint_gap[:, -1].mean().item()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(cur_mean_gap)\n",
    "                \n",
    "            torch.save(model.state_dict(), os.path.join(log_folder_name, f'run{run}', str(epoch)+'_model.pt'))\n",
    "            \n",
    "            if trainer.best_val_objgap > cur_mean_gap:\n",
    "                trainer.patience = 0\n",
    "                trainer.best_val_objgap = cur_mean_gap\n",
    "                trainer.best_val_consgap = cur_cons_gap_mean\n",
    "                trainer.best_val_econsgap = cur_econs_gap_mean\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "  \n",
    "                torch.save(model.state_dict(), os.path.join(log_folder_name, f'run{run}', str(epoch)+'_best_model.pt'))\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(log_folder_name, f'run{run}', str(epoch)+'_model.pt'))\n",
    "    \n",
    "        pbar.set_postfix({'train_loss': train_loss,\n",
    "                          'primal_loss': primal_loss,\n",
    "                          'obj_loss': obj_loss,\n",
    "                          'cons_loss': cons_loss,\n",
    "                          'val_obj': cur_mean_gap,\n",
    "                          'val_cons': cur_cons_gap_mean,\n",
    "                          'val_econs': cur_econs_gap_mean,\n",
    "                          'lr': scheduler.optimizer.param_groups[0][\"lr\"]})\n",
    "        log_dict = {'train_loss': train_loss,\n",
    "                    'primal_loss': primal_loss,\n",
    "                    'obj_loss': obj_loss,\n",
    "                    'cons_loss': cons_loss,\n",
    "                    'val_obj_gap_last_mean': cur_mean_gap,\n",
    "                    'val_cons_gap_last_mean': cur_cons_gap_mean,\n",
    "                    'val_econs_gap_last_mean': cur_econs_gap_mean,\n",
    "                   'lr': scheduler.optimizer.param_groups[0][\"lr\"]}\n",
    "\n",
    "        wandb.log(log_dict)\n",
    "    print('time:', time.time()-curr)\n",
    "\n",
    "    best_val_objgap_mean.append(trainer.best_val_objgap)\n",
    "    best_val_consgap_mean.append(trainer.best_val_consgap)\n",
    "    best_val_econsgap_mean.append(trainer.best_val_econsgap)\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        test_gaps, test_cons_gap, test_gaps_nocon = trainer.eval_metrics_(test_loader, model)\n",
    "\n",
    "    \n",
    "    test_objgap_mean.append(test_gaps[:, -1].mean().item())\n",
    "    test_consgap_mean.append(test_cons_gap[:, -1].mean().item())\n",
    "    test_objgap_nocon_mean.append(test_gaps_nocon[:, -1].mean().item())\n",
    "\n",
    "    wandb.log({'test_objgap': test_objgap_mean[-1]})\n",
    "    wandb.log({'test_consgap': test_consgap_mean[-1]})\n",
    "    wandb.log({'test_objgap_nocon': test_objgap_nocon_mean[-1]})\n",
    "\n",
    "\n",
    "wandb.log({\n",
    "    'best_val_objgap': np.mean(best_val_objgap_mean),\n",
    "    'test_objgap_mean': np.mean(test_objgap_mean),\n",
    "    'test_objgap_std': np.std(test_objgap_mean),\n",
    "    'test_consgap_mean': np.mean(test_consgap_mean),\n",
    "    'test_consgap_std': np.std(test_consgap_mean),\n",
    "    'test_hybrid_gap': np.mean(test_objgap_mean) + np.mean(test_consgap_mean),  # for the sweep\n",
    "})\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca2892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2e3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80520d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db5216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipmgnn",
   "language": "python",
   "name": "ipmgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
